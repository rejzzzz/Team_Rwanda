import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt
import os
import json
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
from experiments.RNN import generate_model

def load_data(file_path):
    print(f"Loading data from {file_path}...")
    
    df = pd.read_csv(file_path)
    
    print(f"Data shape: {df.shape}")
    print("Column names:", df.columns.tolist())
    print("First few rows:")
    print(df.head())
    
    columns_to_drop = ['malware', 'sample_id', 'test_set']
    
    if 'malware' not in df.columns:
        raise ValueError("Target column 'malware' not found in the dataset.")
    
    X = df.drop(columns=columns_to_drop, errors='ignore')
    y = df['malware'].values
    
    print(f"Features shape: {X.shape}")
    print(f"Target shape: {y.shape}")
    print(f"Target distribution: {np.bincount(y)}")
    
    return X, y

def prepare_sequence_data(X, y, sequence_length=10, shuffle=True, random_state=42):
    print(f"Preparing sequence data with sequence length: {sequence_length}...")
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    X_seq = []
    y_seq = []
    
    for i in range(len(X_scaled) - sequence_length):
        X_seq.append(X_scaled[i:i+sequence_length])
        y_seq.append(y[i+sequence_length])
    
    X_seq = np.array(X_seq)
    y_seq = np.array(y_seq)
    
    if shuffle:
        print("Shuffling the sequence data...")
        indices = np.arange(X_seq.shape[0])
        np.random.seed(random_state)
        np.random.shuffle(indices)
        X_seq = X_seq[indices]
        y_seq = y_seq[indices]
    
    num_classes = len(np.unique(y))
    y_seq_cat = to_categorical(y_seq, num_classes=num_classes)
    
    print(f"Sequential data shape - X: {X_seq.shape}, y: {y_seq_cat.shape}")
    
    return X_seq, y_seq_cat

def get_model_parameters():
    parameters = {
        "layer_type": "LSTM",
        "bidirectional": True,
        "depth": 2,
        "hidden_neurons": 64,
        
        "kernel_initializer": "glorot_uniform",
        "recurrent_initializer": "orthogonal",
        
        "dropout": 0.2,
        "recurrent_dropout": 0.2,
        "r_l1_reg": 0.0,
        "r_l2_reg": 0.001,
        "b_l1_reg": 0.0,
        "b_l2_reg": 0.001,
        
        "optimiser": "adam",
        "learning_rate": 0.01,
        "loss": "categorical_crossentropy",
        "activation": "softmax",
        "epochs": 20,
        "sequence_length": 31,
        "batch_size": [64], 
        "description": ["C"],

    }
    
    return parameters

def plot_training_history(history, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'training_history.png'))
    plt.close()
    
    plt.figure(figsize=(14, 8))
    epochs = range(1, len(history.history['loss']) + 1)
    
    train_loss_line, = plt.plot(epochs, history.history['loss'], 'b-', linewidth=2)
    val_loss_line, = plt.plot(epochs, history.history['val_loss'], 'r-', linewidth=2)
    
    for i, (train_loss, val_loss) in enumerate(zip(history.history['loss'], history.history['val_loss'])):
        plt.scatter(i+1, train_loss, c='blue', s=50, zorder=5)
        plt.scatter(i+1, val_loss, c='red', s=50, zorder=5)
        
        plt.annotate(f'{train_loss:.4f}', 
                    (i+1, train_loss),
                    textcoords="offset points", 
                    xytext=(0,10), 
                    ha='center',
                    fontsize=9)
        
        plt.annotate(f'{val_loss:.4f}', 
                    (i+1, val_loss),
                    textcoords="offset points", 
                    xytext=(0,-15), 
                    ha='center',
                    fontsize=9)
    
    plt.title('Training and Validation Loss at Each Epoch', fontsize=16)
    plt.xlabel('Epoch', fontsize=14)
    plt.ylabel('Loss', fontsize=14)
    plt.legend([train_loss_line, val_loss_line], ['Training Loss', 'Validation Loss'], loc='upper right', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    
    plt.ylim(bottom=min(min(history.history['loss']), min(history.history['val_loss'])) * 0.9,
             top=max(max(history.history['loss'][:3]), max(history.history['val_loss'][:3])) * 1.1)
    
    plt.savefig(os.path.join(output_dir, 'detailed_loss_history.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    with open(os.path.join(output_dir, 'training_history.json'), 'w') as f:
        json.dump(history.history, f)
    
    print(f"Training history saved to {output_dir}")

def main():
    data_file = "data.csv"
    sequence_length = 10
    train_size = 0.6
    val_size = 0.2
    test_size = 0.2
    random_state = 42
    batch_size = 32
    epochs = 2
    output_dir = "model_output"
    shuffle_data = True
    
    os.makedirs(output_dir, exist_ok=True)
    
    X, y = load_data(data_file)
    
    X_seq, y_seq = prepare_sequence_data(X, y, sequence_length, 
                                         shuffle=shuffle_data, 
                                         random_state=random_state)
    
    X_temp, X_test, y_temp, y_test = train_test_split(
        X_seq, y_seq, test_size=test_size, random_state=random_state, shuffle=shuffle_data
    )
    
    val_size_adjusted = val_size / (train_size + val_size)
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, shuffle=shuffle_data
    )
    
    print(f"Training set shape - X: {X_train.shape}, y: {y_train.shape}")
    print(f"Validation set shape - X: {X_val.shape}, y: {y_val.shape}")
    print(f"Test set shape - X: {X_test.shape}, y: {y_test.shape}")
    
    total_samples = len(X_seq)
    train_ratio = len(X_train) / total_samples
    val_ratio = len(X_val) / total_samples
    test_ratio = len(X_test) / total_samples
    
    print(f"Actual split ratios - Train: {train_ratio:.2f}, Validation: {val_ratio:.2f}, Test: {test_ratio:.2f}")
    
    parameters = get_model_parameters()
    
    with open(os.path.join(output_dir, 'model_parameters.json'), 'w') as f:
        json.dump(parameters, f, indent=4)
    
    model = generate_model(X_train, y_train, parameters, model_type="rnn")
    
    callbacks = [
        ModelCheckpoint(
            filepath=os.path.join(output_dir, 'best_model.keras'),
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            verbose=1,
            restore_best_weights=True
        )
    ]
    
    print("Training model...")
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=2,
        shuffle=True
    )
    
    print("\nTraining History - Loss per Epoch:")
    print("-" * 50)
    print("Epoch | Training Loss | Validation Loss")
    print("-" * 50)
    for i, (train_loss, val_loss) in enumerate(zip(history.history['loss'], history.history['val_loss'])):
        print(f"{i+1:5d} | {train_loss:.6f}    | {val_loss:.6f}")
    
    plot_training_history(history, output_dir)
    
    print("Evaluating model on validation set...")
    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)
    print(f"Validation Loss: {val_loss:.4f}")
    print(f"Validation Accuracy: {val_accuracy:.4f}")
    
    print("Evaluating model on test set...")
    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {test_accuracy:.4f}")
    
    with open(os.path.join(output_dir, 'evaluation_results.txt'), 'w') as f:
        f.write(f"Validation Loss: {val_loss:.4f}\n")
        f.write(f"Validation Accuracy: {val_accuracy:.4f}\n")
        f.write(f"Test Loss: {test_loss:.4f}\n")
        f.write(f"Test Accuracy: {test_accuracy:.4f}\n")
    
    model.save(os.path.join(output_dir, 'final_model.keras'))
    print(f"Model saved to {os.path.join(output_dir, 'final_model.keras')}")
    
    print("Training completed successfully!")

if __name__ == "__main__":
    main()
