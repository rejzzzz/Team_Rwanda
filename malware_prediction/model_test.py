import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import os
import json

def load_model(model_path):
    """
    Load a saved Keras model.
    
    Args:
        model_path: Path to the saved model file
        
    Returns:
        Loaded Keras model
    """
    print(f"Loading model from {model_path}...")
    model = tf.keras.models.load_model(model_path)
    print("Model loaded successfully.")
    return model

def prepare_data_for_prediction(data, sequence_length=10, scaler=None):
    """
    Prepare data for prediction with the RNN model.
    
    Args:
        data: DataFrame or array of features (without target column)
        sequence_length: Number of time steps in each sequence (must match training)
        scaler: StandardScaler object (if None, a new one will be fitted)
        
    Returns:
        X_seq: Sequential data ready for prediction
        scaler: The StandardScaler object used
    """
    print("Preparing data for prediction...")
    
    # Convert to numpy array if DataFrame
    if isinstance(data, pd.DataFrame):
        X = data.values
    else:
        X = data
        
    # Standardize features
    if scaler is None:
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
    else:
        X_scaled = scaler.transform(X)
    
    # Create sequences
    X_seq = []
    if len(X_scaled) < sequence_length:
        print(f"Warning: Input data has fewer than {sequence_length} samples.")
        # Pad the sequence with zeros if necessary
        padding = np.zeros((sequence_length - len(X_scaled), X_scaled.shape[1]))
        padded_data = np.vstack((padding, X_scaled))
        X_seq.append(padded_data)
    else:
        # Create sequences from the data
        for i in range(len(X_scaled) - sequence_length + 1):
            X_seq.append(X_scaled[i:i+sequence_length])
    
    X_seq = np.array(X_seq)
    print(f"Prepared data shape: {X_seq.shape}")
    return X_seq, scaler

def predict(model, data, sequence_length=10, scaler=None, threshold=0.5):
    """
    Make predictions using the RNN model.
    
    Args:
        model: Trained Keras model
        data: DataFrame or array of features
        sequence_length: Number of time steps in each sequence
        scaler: Optional pre-fitted StandardScaler
        threshold: Probability threshold for binary classification
        
    Returns:
        predictions: Raw prediction probabilities
        predicted_classes: Class predictions (0 or 1)
        prediction_details: DataFrame with detailed results
    """
    X_seq, scaler = prepare_data_for_prediction(data, sequence_length, scaler)
    print("Making predictions...")
    predictions = model.predict(X_seq)
    
    # Get predicted classes
    if predictions.shape[1] > 1:
        predicted_classes = np.argmax(predictions, axis=1)
    else:
        predicted_classes = (predictions > threshold).astype(int)
    
    # Create detailed results
    prediction_details = pd.DataFrame()
    if predictions.shape[1] == 2:
        prediction_details['probability_class_0'] = predictions[:, 0]
        prediction_details['probability_class_1'] = predictions[:, 1]
        prediction_details['predicted_class'] = predicted_classes
    else:
        for i in range(predictions.shape[1]):
            prediction_details[f'probability_class_{i}'] = predictions[:, i]
        prediction_details['predicted_class'] = predicted_classes
    
    return predictions, predicted_classes, prediction_details

def batch_predict_from_file(model_path, data_file, output_dir="prediction_results", sequence_length=10):
    """
    Load data from a file and make predictions using a saved model.
    
    Args:
        model_path: Path to the saved model
        data_file: Path to the data CSV file
        output_dir: Directory to save prediction results
        sequence_length: Number of time steps in each sequence
        
    Returns:
        DataFrame with prediction results
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Load model (this loads the model again; if needed, you can pass an already loaded model)
    model = load_model(model_path)
    
    # Load data
    print(f"Loading data from {data_file}...")
    df = pd.read_csv(data_file)
    
    # Extract sample IDs if available
    sample_ids = None
    if 'sample_id' in df.columns:
        sample_ids = df['sample_id'].values
    
    # Check for target column
    has_target = 'malware' in df.columns
    
    # Prepare features
    if has_target:
        columns_to_drop = ['malware', 'sample_id', 'test_set']
        y_true = df['malware'].values
        X = df.drop(columns=columns_to_drop, errors='ignore')
    else:
        columns_to_drop = ['sample_id', 'test_set']
        y_true = None
        X = df.drop(columns=columns_to_drop, errors='ignore')
    
    # Make predictions
    _, predicted_classes, prediction_details = predict(model, X, sequence_length)
    
    # Add sample IDs to results
    if sample_ids is not None:
        if len(predicted_classes) != len(sample_ids):
            offset = sequence_length - 1
            adjusted_sample_ids = sample_ids[offset:] if len(sample_ids) > offset else sample_ids
            prediction_details['sample_id'] = adjusted_sample_ids
        else:
            prediction_details['sample_id'] = sample_ids
    
    # Add true labels if available and calculate accuracy
    if y_true is not None:
        if len(predicted_classes) != len(y_true):
            offset = sequence_length - 1
            adjusted_y_true = y_true[offset:] if len(y_true) > offset else y_true
            prediction_details['true_class'] = adjusted_y_true
            accuracy = np.mean(predicted_classes == adjusted_y_true)
            print(f"Prediction accuracy: {accuracy:.4f}")
        else:
            prediction_details['true_class'] = y_true
            accuracy = np.mean(predicted_classes == y_true)
            print(f"Prediction accuracy: {accuracy:.4f}")
    
    # Save results
    results_file = os.path.join(output_dir, 'prediction_results.csv')
    prediction_details.to_csv(results_file, index=False)
    print(f"Prediction results saved to {results_file}")
    
    return prediction_details

def predict_single_sample(model_path, feature_values, sequence_length=10, scaler_path=None):
    """
    Make a prediction for a single sample.
    
    Args:
        model_path: Path to the saved model
        feature_values: List or array of feature values for a single sample
        sequence_length: Number of time steps in each sequence
        scaler_path: Optional path to a saved StandardScaler
        
    Returns:
        Prediction result for the sample
    """
    model = load_model(model_path)
    
    # Load scaler if provided
    scaler = None
    if scaler_path and os.path.exists(scaler_path):
        print(f"Loading scaler from {scaler_path}...")
        with open(scaler_path, 'rb') as f:
            import pickle
            scaler = pickle.load(f)
    
    # Ensure feature_values is 2D (even for a single sample)
    if len(np.array(feature_values).shape) == 1:
        feature_values = np.array(feature_values).reshape(1, -1)
    
    # For a single sample, we need a sequence so we create a synthetic sequence by repeating the sample
    print("For a single sample, you would typically need previous samples to form a sequence.")
    print("Creating a synthetic sequence for demonstration purposes...")
    sample_repeated = np.tile(feature_values, (sequence_length, 1))
    
    X_seq, _ = prepare_data_for_prediction(sample_repeated, sequence_length, scaler)
    predictions = model.predict(X_seq)
    
    # Determine predicted class and probabilities
    if predictions.shape[1] > 1:
        predicted_class = np.argmax(predictions[0])
        class_probabilities = {f"Class {i}": float(prob) for i, prob in enumerate(predictions[0])}
    else:
        predicted_class = 1 if predictions[0][0] > 0.5 else 0
        class_probabilities = {"Class 0": float(1 - predictions[0][0]), "Class 1": float(predictions[0][0])}
    
    result = {
        "predicted_class": int(predicted_class),
        "probabilities": class_probabilities,
    }
    
    return result

if __name__ == "__main__":
    # Define paths for the model and data
    model_path = "model_output/best_model.keras"
    data_file = "test_data.csv"
    
    # Load the model and print its summary
    print("Loading model for summary...")
    model = load_model(model_path)
    print("Model summary:")
    model.summary()
    
    # Example 1: Batch prediction from file
    print("\nStarting batch prediction from file...")
    batch_results = batch_predict_from_file(model_path, data_file, sequence_length=10)
    
    # Example 2: Single sample prediction (detailed features)
    sample_features = [
        0,          #vector
        6.5E+08,    # memory
        1066,      # tx_packets
        3317908,   # rx_bytes
        6.94E+08,   # swap
        2726,      # rx_packets
        0.204887,   # cpu_sys
        49,         # total_pro
        0.088346,   # cpu_user
        3036,       # max_pid
        713908,   # tx_bytes
        1,         #predicted class
    ]
    print("\nStarting single sample prediction (2detailed features)...")
    single_prediction = predict_single_sample(model_path, sample_features, sequence_length=10)
    print("Single sample prediction (detailed features):")
    print(json.dumps(single_prediction, indent=2))
    
    # Example 3: Single sample prediction (simplified features)
    sample_features_simple = [0.5, 0.2, 0.3, 0.1, 0.7, 0.0, 0.9, 0.2, 0.5, 0.3, 41214, 0]
    print("\nStarting single sample prediction (simplified features)...")
    single_prediction_simple = predict_single_sample(model_path, sample_features_simple, sequence_length=10)
    print("Single sample prediction (simplified features):")
    print(json.dumps(single_prediction_simple, indent=2))